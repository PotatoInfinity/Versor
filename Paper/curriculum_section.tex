\subsection{Curriculum Generalization: Robustness to Length Scaling}

To evaluate architectural robustness to distribution shifts, we trained all models on short sequences (L=50) and evaluated zero-shot transfer to longer sequences (L=100, 150). This tests whether learned dynamics are tied to specific sequence lengths or capture scale-invariant relationships. We measure the \textbf{Transfer Gap}: the ratio of test MSE at a given length to the MSE at the training length.

\begin{table}[h]
\centering
\caption{Transfer Gap across sequence lengths. Lower is better. Models trained on L=50, tested on longer sequences.}
\begin{tabular}{lccc}
\toprule
Model & Training MSE & L=100 Gap & L=150 Gap \\
\midrule
Transformer & 3.12 & \textbf{1.93$\times$} & \textbf{2.62$\times$} \\
Versor & 0.07 & 9.47$\times$ & \textbf{11.44$\times$} \\
GNS & 0.12 & \textbf{450.11$\times$} & 28.53$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:} GNS exhibits catastrophic failure when generalizing beyond the training sequence length, with performance collapsing by 450$\times$ at twice the training length (L=100, MSE=55.4 vs 0.12 at L=50) before partially recovering at L=150. This occurs because graph-based models learn position-dependent edge features and fixed connectivity patterns that fail to transfer across temporal scales.

Versor demonstrates significantly more robust generalization than GNS (11$\times$ vs 450$\times$ degradation, a \textbf{40$\times$ improvement}), suggesting that geometric relationships provide better inductive biases than graph structure for length generalization. However, Transformer's positional encodings remain most effective for this particular task (2.6$\times$ degradation), indicating that explicit temporal modeling complements geometric priors.

This result highlights a key architectural tradeoff: GNS achieves lowest MSE on in-distribution data (Table 2) but fails catastrophically out-of-distribution, while Versor trades marginal in-distribution performance for 40$\times$ better robustness. For applications requiring transfer across scales (e.g., training on short simulations, deploying on long-horizon predictions), Versor's geometric structure provides crucial stability.

\textbf{Implication:} The catastrophic failure of graph-based methods on curriculum generalization motivates Versor's sequence-based geometric approach. While absolute best performance remains an open challenge (with Transformer's positional encodings proving most effective here), Versor occupies a unique niche: better than GNS at generalization, more interpretable than Transformers, and with 6.6$\times$ fewer parameters than both.
